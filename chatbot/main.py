from src.prompts import TCprompts
from src.vec_db.utils import search_through_chunks_collection
from src.embeddings import Embeddings
from fastapi import FastAPI, status
from fastapi.responses import JSONResponse
import os
import uvicorn
from constants import SENTRY_DNS, SENTRY_TRACE_SAMPLE_RATE
import sentry_sdk
from sentry_sdk import start_span
from dotenv import load_dotenv
from constants import LLM_MAX_NAME_EXTRACT_LIMIT, LLM_MAX_NAME_EXTRACT_INPUT_TOKEN
from src.inference import DAModelInference

load_dotenv()  # Load variables from .env file

app = FastAPI()
sentry_sdk.init(
    dsn=SENTRY_DNS,
    # To adjust sampling, change the value (0.0 to 1.0)
    traces_sample_rate= SENTRY_TRACE_SAMPLE_RATE,  # Be cautious with a rate of 1.0 in production
)


# Initialize embedding here to avoid multiple initializations
embedding = Embeddings()

@app.post("/data_gov_chat")
async def data_gov_chat(data: dict):
    """
    API endpoint for doc chat.
    
    Parameters:
    - data (dict): A dictionary containing the document ID and user question.

    Returns:
    - JSONResponse: A response object containing the answer generated by the TCInference model.
    """
    with start_span(op="tally-chat-docs", description="tally-chat-docs"):
        # Get the doc id
        # data = request.json
        question = data.get("user_question")
        
        # checking if document exists in db 
        # chunk_count = count_chunks_for_doc_id(doc_id)
            
        # if chunk_count == 0 or chunk_count == None:
        #     DocumentProcessor(doc_id).embed_and_send_to_db()

        # Embed for searching through vector db
        aggregated_dense_embedding, aggregated_sparse_embedding = embedding.encode_for_search(question)

        # get data from vector db
        data = search_through_chunks_collection(aggregated_sparse_embedding, aggregated_dense_embedding)

        print(data)
        # Get title and text context
        context = [f"Description:\t{str(chunk.entity.text)}" for chunk_list in data for chunk in chunk_list]
        context = "\n".join(context)

        # Get the doc content from database
        # doc_content = get_document_text_from_chunk_id(data = data, doc_id = doc_id)


        # Get the context prompt
        context_prompt = TCprompts(content=context, question=question).context_prompt()

        model_inference = DAModelInference()._check_and_assign_inference_model("openai")

        # perform inference
        answer, _, _ = model_inference(context_prompt)
        return JSONResponse(status_code=status.HTTP_200_OK, content={"answer": answer})
    

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))